---
title: Spark Connect
description: Learn how to utilize the provided PySpark scripts and notebook to connect to a Spark Connect cluster hosted on IOMETE for efficient data processing and analysis.
last_update:
  date: 03/12/2024
  author: Nurlan Mammadov
---

import { Plus, GithubLogo } from "@phosphor-icons/react";

import Card from "@site/src/components/Card";

import Img from "@site/src/components/Img";

In Apache Spark > 3.4, Spark Connect introduced a decoupled client-server architecture that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks and programming languages.

---

## **Create a new Cluster**

**1.** Go to the **Spark Connect** and click the <button className="button button--primary button-iom"><Plus size={16}/>Create</button> button

<Img src="/img/user-guide/spark-connect/clusters.png" alt="Clusters" />
<br />

**2.** Give the new **cluster** a name under **Name**.

<Img
  src="/img/user-guide/spark-connect/cluster-create.png"
  alt="Create Spark Cluster"
/>
<br />

**3.** SelectÂ driver, under the **Node driver** section. [Read more](./node-types.md) about node types.

<Img
  src="/img/user-guide/spark-connect/cluster-driver-select.png"
  alt="Cluster Node Driver"
  maxWidth="500px"
/>

:::info Node driver
Spark driver is running all the time until cluster stopped manually. Driver is responsible for managing executors/workers and connections. If stopped, no connections could be established to the cluster.
:::

<br />

**4.** SelectÂ executor, under the **Node executor** section. [Read more](https://spark.apache.org/docs/latest/cluster-overview.html) about spark executors.

<Img
  src="/img/user-guide/spark-connect/cluster-executor-select.png"
  alt="Cluster executor select"
  maxWidth="500px"
/>

<br />

**5.** InputÂ executor count, under the **Executor count** section.

Below these inputs, a real-time preview of **Total CPU** and **Total Memory** is provided. This information helps you make informed decisions about the selection of Node Executors and the number of Executors. It ensures that you allocate sufficient resources to meet the demands of your workload.

<Img
  src="/img/user-guide/spark-connect/cluster-executor.png"
  alt="Cluster executor"
  maxWidth="500px"
/>

:::info Node executor
Executors basically are responsible for executing the queries. They will be scaled up and down automatically based on the auto-suspend parameter.

Keep **auto-suspend on** to minimize clusters costs.
:::

<br />

**6.** SelectÂ volume, under the **Volume** section. [Read more](./volumes.md) about volumes.

<Img
  src="/img/user-guide/virtual-lakehouse/lakehouse-volume-select.png"
  alt="Cluster volume select"
  maxWidth="500px"
/>

**7.** Set Auto suspend under **Auto suspend** section.

By clicking checkbox in the left side we can **disabled Auto suspend** functionality.

<Img
  src="/img/user-guide/spark-connect/cluster-auto-suspend.png"
  alt="Cluster auto suspend"
  maxWidth="500px"
/>

:::info
Executors will be scaled down after the specified time of inactivity. Executors will be scaled up automatically on demand (Scale up time around 10-15 seconds). It is recommended to keep auto-suspend on to minimize monthly costs.
:::

<br />

**8.** Click the <button className="button button--primary button-iom">Create</button> button after adding a **description** to the **optional description field**.

<Img
  src="/img/user-guide/spark-connect/cluster-desc.png"
  alt="Cluster Description"
  maxWidth="500px"
/>
<br />

ðŸŽ‰ ðŸŽ‰ðŸŽ‰Â **Tadaa**! The newly created **test-cluster** details view is shown.

## **Cluster details**

<Img
  src="/img/user-guide/spark-connect/cluster-details.png"
  alt="Cluster Details"
/>

1.  Navigation buttons - **Spark UI** - this button will take us Spark Jobs information. - **Edit** - this button will take us to the editing form. - **Terminate / Start / Restart** - buttons for the cluster's terminate, start and restart.
2.  For detailed information on statuses, please refer to the [Lakehouse Status](./virtual-lakehouses#lakehouse-cluster-statuses) documentation section.

3.  General information.

4.  In the **Connections** section, you can copy the **endpoint** for the connection.

5.  **Events**
    In this section we may check your cluster's **Start**/**Terminate** events.

    <Img
      src="/img/user-guide/spark-connect/cluster-events.png"
      alt="Cluster events"
      maxWidth="500px"
    />

6.  **Delete** - this button makes it simple to **delete** Cluster.

---

<Card
  title="Spark Connect Quickstart Guide"
  icon={<GithubLogo />}
  link="https://github.com/iomete/spark-connect-quickstart"
>
  Jumpstart your journey with Spark Connect by using our Quickstart Guide. This
  repository includes hands-on PySpark Python and notebook examples, along with
  detailed instructions on how to use Spark Connect in your projects.
</Card>
